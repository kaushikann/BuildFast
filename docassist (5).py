# -*- coding: utf-8 -*-
"""DocAssist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uXXcuxF9xKUgQRV3XO8erNX0S81UfjQI
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-openai==0.2.12

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-pinecone

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain_community pypdf

import os

os.environ["OPENAI_API_KEY"]=st.secrets["OPENAI_API_KEY"]

from langchain_openai import ChatOpenAI

gpt4omini = ChatOpenAI(
    model_name="gpt-4o-mini", temperature=0)

from langchain import PromptTemplate


from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

msg = SystemMessage(content="You are an helpful assistant. Your name is Bummy. You are an educational tutor to high school students")

from langchain.memory import ConversationBufferMemory

memo = ConversationBufferMemory(k=15)

from langchain import ConversationChain

convo = ConversationChain(llm=gpt4omini, memory=memo)

from langchain_openai import OpenAIEmbeddings

embeds = OpenAIEmbeddings(model="text-embedding-3-small")

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1024, chunk_overlap=64)

from langchain_pinecone import PineconeVectorStore

import os
os.environ['PINECONE_API_KEY'] = st.secrets["PINECONE_API_KEY"]

import streamlit as st

from langchain import LLMChain

from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA

st.header("Hi, I am Bummy - your AI Assistant.")
st.subheader("Please upload a document for me to answer your questions")
chapter_doc=st.file_uploader("Upload", type="pdf",label_visibility="visible")
texts = ""
for pdf in chapter_doc:
  pdf_reader = PdfReader(chapter_doc)
  for page in pdf_reader.pages:
    texts += page.extract_text()

doc_split = text_splitter.split_text(texts)

doc_embed = embeds.embed_documents(doc_split)
index="chapter"
vectorstore = PineconeVectorStore.from_documents(doc_split, doc_embed, index_name=index)

llm = ChatOpenAI(model="gpt-4o", temperature=0)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

prmpt = st.text_area("What is your question?",label_visibility="visible")
results = vectorstore.similarity_search(prmpt,k=2)
result1=results[0].page_content
result2=results[1].page_content

prmpt2_template = "Using {result1} and {result2} answer the question {prmpt}"
prmpt2_prompt = PromptTemplate(template = prmpt2_template, input_variables = ['result1', 'result2', 'prmpt'])

doc_chain = prmpt2_prompt | gpt4omini

finalresponse = doc_chain.invoke({"result1" : result1, "result2" : result2, "prmpt" : prmpt})
st.write(finalresponse)