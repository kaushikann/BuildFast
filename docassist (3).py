# -*- coding: utf-8 -*-
"""DocAssist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uXXcuxF9xKUgQRV3XO8erNX0S81UfjQI
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-openai==0.2.12

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-pinecone

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

import os

os.environ["OPENAI_API_KEY"]=""

from langchain_openai import ChatOpenAI

gpt4omini = ChatOpenAI(
    model_name="gpt-4o-mini", temperature=0)

from langchain import PromptTemplate


from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

msg = SystemMessage(content="You are an helpful assistant. Your name is Bummy. You are an educational tutor to high school students")

from langchain.memory import ConversationBufferMemory

memo = ConversationBufferMemory(k=15)

from langchain import ConversationChain

convo = ConversationChain(llm=gpt4omini, memory=memo)

from langchain_openai import OpenAIEmbeddings

embeds = OpenAIEmbeddings(model="text-embedding-3-small")

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=100)

from langchain_pinecone import PineconeVectorStore

import os
os.environ['PINECONE_API_KEY'] = ''

import streamlit as st

from langchain import LLMChain

st.header("Hi, I am Bummy - your AI Assistant.")
st.subheader("Please upload a document for me to answer your questions")
chapter_doc=st.file_uploader("Upload", type="pdf",label_visibility="visible")
chapter_read = chapter_doc.read()

doc_split = text_splitter.split_text(chapter_read)
doc_ready= doc_split.create_documents(doc_split)
doc_embed = embeds.embed_documents(doc_ready)
index="chapter"
vectorstore = PineconeVectorStore.from_documents(doc_ready, doc_embed, index_name=index)

prmpt = st.text_area("What is your question?",label_visibility="visible")
results = vectorstore.similarity_search(prmpt,k=2)
result1=results[0].page_content
result2=results[1].page_content

prmpt2_template = "Using {result1} and {result2} answer the question {prmpt}"
prmpt2_prompt = PromptTemplate(template = prmpt2_template, input_variables = ['result1', 'result2', 'prmpt'])

doc_chain = prmpt2_prompt | gpt4omini

finalresponse = doc_chain.invoke({"result1" : result1, "result2" : result2, "prmpt" : prmpt})
st.write(finalresponse)